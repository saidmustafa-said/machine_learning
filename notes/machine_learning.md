<!-- @format -->

Here's a detailed Markdown study guide explaining each step, why we did it, and the different models used. Let me know if you want more details or formatting tweaks! ğŸš€

---

### ğŸ¡ House Price Prediction Study Guide

This guide walks through building and improving a model to predict house prices using machine learning. We'll explore:

- **Data exploration and preprocessing**
- **Decision Tree regression**
- **Model evaluation using Mean Absolute Error (MAE)**
- **Hyperparameter tuning (max_leaf_nodes)**
- **Using Random Forest for better accuracy**

## ğŸ“Œ Part 1: Loading and Understanding the Data

Before training a model, we must load and understand the dataset.

```python
import pandas as pd

# Path to dataset
iowa_file_path = '../input/home-data-for-ml-course/train.csv'

# Load data
home_data = pd.read_csv(iowa_file_path)

# Show summary statistics
home_data.describe()
```

### ğŸ”¹ Why This Step?

- `pd.read_csv()` loads the dataset.
- `.describe()` gives a summary of numerical data (mean, min, max, etc.), helping us understand the dataset.

---

## ğŸ“Œ Part 2: Selecting the Target and Features

### **Target Variable (y)**

The variable we want to predict is **SalePrice**.

```python
y = home_data["SalePrice"]
```

### **Selecting Features (X)**

We choose relevant features that influence house prices:

```python
feature_names = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
X = home_data[feature_names]
```

### ğŸ”¹ Why These Features?

- **LotArea**: Larger lots may increase value.
- **YearBuilt**: Older homes might be less valuable unless renovated.
- **1stFlrSF & 2ndFlrSF**: Total floor space affects price.
- **FullBath, BedroomAbvGr, TotRmsAbvGrd**: More rooms and bathrooms generally mean higher prices.

---

## ğŸ“Œ Part 3: Training a Decision Tree Model

A **Decision Tree Regressor** predicts house prices based on input features.

```python
from sklearn.tree import DecisionTreeRegressor

# Initialize model
iowa_model = DecisionTreeRegressor(random_state=1)

# Train the model
iowa_model.fit(X, y)

# Make predictions
predictions = iowa_model.predict(X)

# Show first few predictions
print(predictions[:5])
```

### ğŸ”¹ Why Decision Tree?

- **Easy to interpret**
- **Handles non-linear relationships**
- **Quick to train**

**But:** It may **overfit** (perform well on training data but poorly on new data).

---

## ğŸ“Œ Part 4: Splitting Data for Better Evaluation

Instead of training on all data, we split into:

- **Training set** (used for learning)
- **Validation set** (used for testing)

```python
from sklearn.model_selection import train_test_split

# Split data into training and validation sets
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
```

### ğŸ”¹ Why Split Data?

- **Prevents overfitting** (model memorizing data instead of generalizing)
- **Helps assess real-world performance**

---

## ğŸ“Œ Part 5: Evaluating the Model with MAE

To measure accuracy, we use **Mean Absolute Error (MAE)**:

```python
from sklearn.metrics import mean_absolute_error

# Train the model on training data
iowa_model.fit(train_X, train_y)

# Predict on validation data
val_predictions = iowa_model.predict(val_X)

# Calculate MAE
val_mae = mean_absolute_error(val_y, val_predictions)
print(f"Validation MAE: {val_mae}")
```

### ğŸ”¹ Why MAE?

- Measures how far predictions are from actual prices.
- Lower MAE = Better model performance.

**Downside:** It treats all errors equally (no penalty for large errors).

---

## ğŸ“Œ Part 6: Improving Decision Tree with Hyperparameter Tuning

We can **optimize max_leaf_nodes** to balance performance and overfitting.

```python
def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return mae

# Test different values
candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]
mae_scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}

# Best tree size
best_tree_size = min(mae_scores, key=mae_scores.get)
print(f"Best tree size: {best_tree_size}")
```

```python
# Train final model with best max_leaf_nodes
final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)
final_model.fit(X, y)
```

### ğŸ”¹ Why Tune max_leaf_nodes?

- **Too few leaves** â†’ Underfitting (missing patterns).
- **Too many leaves** â†’ Overfitting (memorizing data).

---

## ğŸ“Œ Part 7: Using Random Forest for Better Accuracy

A **Random Forest** is a collection of multiple decision trees, improving accuracy.

```python
from sklearn.ensemble import RandomForestRegressor

# Initialize model
rf_model = RandomForestRegressor(random_state=1)

# Train model
rf_model.fit(train_X, train_y)

# Predict on validation data
rf_val_predictions = rf_model.predict(val_X)

# Calculate MAE
rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)
print(f"Validation MAE for Random Forest Model: {rf_val_mae}")
```

### ğŸ”¹ Why Use Random Forest?

âœ… **Reduces overfitting** (averaging multiple trees prevents memorization).  
âœ… **Handles complex patterns better than a single tree**.  
âœ… **More stable and accurate predictions**.

---

## ğŸ“Œ Comparison of Models

| Model               | Validation MAE (lower is better) | Strengths                          | Weaknesses                  |
| ------------------- | -------------------------------- | ---------------------------------- | --------------------------- |
| Decision Tree       | Higher                           | Simple, interpretable              | Overfits easily             |
| Tuned Decision Tree | Lower                            | Better than basic tree             | Still overfits sometimes    |
| Random Forest       | Lowest                           | Best accuracy, reduces overfitting | Slower, harder to interpret |

---

## ğŸ¯ Final Thoughts

1. **Decision Trees** are great but prone to **overfitting**.
2. **Tuning max_leaf_nodes** helps balance bias vs. variance.
3. **Random Forests** give the **best accuracy** by combining multiple trees.

ğŸ“Œ **Key Takeaways:**

- **Always split data into training & validation sets**.
- **Use MAE to evaluate models**.
- **Experiment with different models for better results**.

---

ğŸ”¹ **Next Steps:**  
Try improving further by:

- Adding more features.
- Using different algorithms (e.g., Gradient Boosting).
- Normalizing data for better performance.

Hope this helps! ğŸš€ Let me know if you need any refinements! ğŸ˜Š
