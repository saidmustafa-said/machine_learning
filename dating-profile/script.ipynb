{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report for the okcupid_profile dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from fastFM import sgd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/okcupid_profiles.csv'\n",
    "data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59946 entries, 0 to 59945\n",
      "Data columns (total 31 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   age          59946 non-null  int64  \n",
      " 1   status       59946 non-null  object \n",
      " 2   sex          59946 non-null  object \n",
      " 3   orientation  59946 non-null  object \n",
      " 4   body_type    54650 non-null  object \n",
      " 5   diet         35551 non-null  object \n",
      " 6   drinks       56961 non-null  object \n",
      " 7   drugs        45866 non-null  object \n",
      " 8   education    53318 non-null  object \n",
      " 9   ethnicity    54266 non-null  object \n",
      " 10  height       59943 non-null  float64\n",
      " 11  income       59946 non-null  int64  \n",
      " 12  job          51748 non-null  object \n",
      " 13  last_online  59946 non-null  object \n",
      " 14  location     59946 non-null  object \n",
      " 15  offspring    24385 non-null  object \n",
      " 16  pets         40025 non-null  object \n",
      " 17  religion     39720 non-null  object \n",
      " 18  sign         48890 non-null  object \n",
      " 19  smokes       54434 non-null  object \n",
      " 20  speaks       59896 non-null  object \n",
      " 21  essay0       54458 non-null  object \n",
      " 22  essay1       52374 non-null  object \n",
      " 23  essay2       50308 non-null  object \n",
      " 24  essay3       48470 non-null  object \n",
      " 25  essay4       49409 non-null  object \n",
      " 26  essay5       49096 non-null  object \n",
      " 27  essay6       46175 non-null  object \n",
      " 28  essay7       47495 non-null  object \n",
      " 29  essay8       40721 non-null  object \n",
      " 30  essay9       47343 non-null  object \n",
      "dtypes: float64(1), int64(2), object(28)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from fastFM import sgd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy import sparse\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load the dataset\n",
    "# -----------------------------\n",
    "# Replace 'dating_app_data.csv' with your actual filename/path.\n",
    "df = pd.read_csv('dating_app_data.csv')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Combine Essay Columns\n",
    "# -----------------------------\n",
    "# Assume the essay columns are named 'essay0', 'essay1', ..., 'essay9'\n",
    "essay_cols = [f'essay{i}' for i in range(10)]\n",
    "# Fill missing essays with an empty string and combine them into one large text field.\n",
    "df['essays'] = df[essay_cols].fillna('').agg(' '.join, axis=1)\n",
    "# Drop the original essay columns (no longer needed)\n",
    "df.drop(columns=essay_cols, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Clean Missing Values\n",
    "# -----------------------------\n",
    "# List of categorical columns (adjust as needed)\n",
    "categorical_cols = ['status', 'sex', 'orientation', 'body_type', 'diet',\n",
    "                    'drinks', 'drugs', 'education', 'ethnicity', 'job',\n",
    "                    'location', 'offspring', 'pets', 'religion', 'sign',\n",
    "                    'smokes', 'speaks']\n",
    "\n",
    "# For categorical columns, fill missing values with a placeholder (e.g., \"unknown\")\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"unknown\")\n",
    "\n",
    "# List of numeric columns that we plan to use\n",
    "numeric_cols = ['age', 'height', 'income']\n",
    "# For numeric columns, fill missing values with the median value\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Outlier Detection and Removal\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def remove_outliers(df, columns):\n",
    "    \"\"\"Remove rows where any of the specified numeric columns is an outlier.\n",
    "       Outliers are defined using the IQR method (beyond 1.5 * IQR).\"\"\"\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = remove_outliers(df, numeric_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Create a Target Variable\n",
    "# -----------------------------\n",
    "# For demonstration, assume that if a user's status is \"single\" (case-insensitive)\n",
    "# then they are available for matching (target=1), otherwise 0.\n",
    "df['target'] = (df['status'].str.lower() == 'single').astype(int)\n",
    "# Since we used \"status\" to create the target, drop it from the features.\n",
    "df.drop(columns=['status'], inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Feature Engineering & Preprocessing Setup\n",
    "# -----------------------------\n",
    "# Define the features to be processed:\n",
    "numeric_features = ['age', 'height', 'income']\n",
    "# Note: Exclude 'status' since it was used for target creation.\n",
    "categorical_features = ['sex', 'orientation', 'body_type', 'diet',\n",
    "                        'drinks', 'drugs', 'education', 'ethnicity', 'job',\n",
    "                        'location', 'offspring', 'pets', 'religion', 'sign',\n",
    "                        'smokes', 'speaks']\n",
    "text_feature = 'essays'  # our combined essay text\n",
    "\n",
    "# Create a ColumnTransformer that:\n",
    "# - Standardizes numeric features,\n",
    "# - One-hot encodes categorical features,\n",
    "# - Vectorizes the text (using TF–IDF) with a maximum of 1000 features.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore',\n",
    "         sparse=True), categorical_features),\n",
    "        ('text', TfidfVectorizer(max_features=1000), text_feature)\n",
    "    ],\n",
    "    remainder='drop'  # drop any columns not specified above\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the entire dataset and transform the data.\n",
    "X = preprocessor.fit_transform(df)\n",
    "y = df['target'].values\n",
    "\n",
    "# fastFM works best with sparse input; if X isn’t already sparse, convert it.\n",
    "if not sparse.issparse(X):\n",
    "    X = sparse.csr_matrix(X)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Split the Dataset\n",
    "# -----------------------------\n",
    "# First split into training (70%) and temporary (30%) sets, stratifying by target.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Then split the temporary set equally into test and evaluation sets (15% each of the original data).\n",
    "X_test, X_eval, y_test, y_eval = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Train Factorization Machines (FM) Model\n",
    "# -----------------------------\n",
    "# We use the SGD-based FMClassification from fastFM.\n",
    "# Adjust hyperparameters (n_iter, rank, regularization, etc.) as needed.\n",
    "fm = sgd.FMClassification(n_iter=100,\n",
    "                          init_stdev=0.1,\n",
    "                          rank=8,\n",
    "                          l2_reg_w=0.1,\n",
    "                          l2_reg_V=0.5,\n",
    "                          random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "fm.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Evaluate the Model\n",
    "# -----------------------------\n",
    "# Predict probabilities on the test set. (fastFM returns continuous outputs; threshold at 0.5 for binary decisions.)\n",
    "y_test_pred = fm.predict(X_test)\n",
    "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_binary)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"  Accuracy: {:.4f}\".format(test_accuracy))\n",
    "print(\"  ROC AUC:  {:.4f}\".format(test_roc_auc))\n",
    "\n",
    "# Similarly, evaluate on the evaluation set\n",
    "y_eval_pred = fm.predict(X_eval)\n",
    "y_eval_pred_binary = (y_eval_pred > 0.5).astype(int)\n",
    "eval_accuracy = accuracy_score(y_eval, y_eval_pred_binary)\n",
    "eval_roc_auc = roc_auc_score(y_eval, y_eval_pred)\n",
    "\n",
    "print(\"\\nEvaluation Set Evaluation:\")\n",
    "print(\"  Accuracy: {:.4f}\".format(eval_accuracy))\n",
    "print(\"  ROC AUC:  {:.4f}\".format(eval_roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
